# -*- coding: utf-8 -*-
"""Final_Project_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11kXHQanOfKiri8T1FDnHZcVqs6qDdLKu

# **PIPELINE1**
"""

# Extracts text from XML papers (converted locally via GROBID)
# and generates structured summaries using the BART-Large-CNN model.
from pathlib import Path
from bs4 import BeautifulSoup

xml_path = Path("/content/2601.04110v1.xml")
JSON_output_path= xml_path.with_suffix(".json")
txt_output_path = xml_path.with_name(xml_path.stem + "_summary.txt")
pipeline4_output_path = xml_path.with_name(
    xml_path.stem + "_with_figures.txt"
)

with open(xml_path, "r", encoding="utf-8") as f:
    xml_content = f.read()

soup = BeautifulSoup(xml_content, "xml")

# Title
title_tag = soup.find("titleStmt")
title = title_tag.find("title").get_text(strip=True) if title_tag else "N/A"

# Authors
authors = []
for author in soup.find_all("author"):
    pers = author.find("persName")
    if pers:
        authors.append(pers.get_text(" ", strip=True))

authors_text = ", ".join(authors) if authors else "N/A"

# Date
date_tag = soup.find("date")
date = date_tag.get_text(strip=True) if date_tag else "N/A"

print("Title:", title)
print("Authors:", authors_text)
print("Date:", date)

abstract = soup.find("abstract")
abstract_text = abstract.get_text(" ", strip=True) if abstract else ""

body = soup.find("body")
body_text = body.get_text(" ", strip=True) if body else ""

full_text = abstract_text + "\n\n" + body_text
len(full_text)

import re
# Clean extracted text by removing extra spaces and citation references
def clean_text(text):
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\[[0-9,\s]+\]", "", text)
    return text.strip()

cleaned_text = clean_text(full_text)

# Split long text into chunks suitable for BART-Large-CNN input limits
def chunk_text(text, max_words=450):
    words = text.split()
    chunks = []
    for i in range(0, len(words), max_words):
        chunk = " ".join(words[i:i + max_words])
        if len(chunk) > 120:
            chunks.append(chunk)
    return chunks

chunks = chunk_text(cleaned_text)
len(chunks)

from transformers import pipeline
# Initialize BART-Large-CNN summarization model
summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn",
    device=-1
)
# Generate summaries for each text chunk
raw_summaries = []

for i, chunk in enumerate(chunks):
    s = summarizer(
        chunk,
        max_length=150,
        min_length=60,
        do_sample=False
    )
    raw_summaries.append(s[0]["summary_text"])
    print(f"✅ Chunk {i+1} summarized")

raw_summary = " ".join(raw_summaries)

# Split the final summary into structured academic sections
def split_structured(summary):
    sentences = summary.split(". ")
    n = len(sentences)

    return {
        "Introduction": ". ".join(sentences[:n//6]),
        "Problem": ". ".join(sentences[n//6:2*n//6]),
        "Proposed Method": ". ".join(sentences[2*n//6:3*n//6]),
        "Key Contributions": ". ".join(sentences[3*n//6:4*n//6]),
        "Experimental Results": ". ".join(sentences[4*n//6:5*n//6]),
        "Conclusion": ". ".join(sentences[5*n//6:])
    }

section_summaries = split_structured(raw_summary)

final_output = f"""
Title:
{title}

Authors:
{authors}

Date:
{date}

Introduction:
{section_summaries['Introduction']}

Problem:
{section_summaries['Problem']}

Proposed Method:
{section_summaries['Proposed Method']}

Key Contributions:
{section_summaries['Key Contributions']}

Experimental Results:
{section_summaries['Experimental Results']}

Conclusion:
{section_summaries['Conclusion']}
"""

with open(txt_output_path, "w", encoding="utf-8") as f:
    f.write(final_output)

print(f"✅ TXT summary saved as: {txt_output_path}")

"""**HTML**"""

# HTML GENERATION.


def generate_html_from_summary(
    xml_path,
    title,
    authors,
    date,
    section_summaries
):
    output_html_path = xml_path.with_suffix(".html")

    authors_display = ", ".join(authors[:5])
    if len(authors) > 5:
        authors_display += " et al."

    html = []

    html.append("""
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Paper Summary</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 40px;
                color: #222;
                background-color: #ffffff;
            }
            h1 {
                border-bottom: 2px solid #333;
                padding-bottom: 10px;
            }
            h2 {
                margin-top: 40px;
                color: #003366;
            }
            .meta {
                margin-top: 10px;
                margin-bottom: 30px;
                font-size: 14px;
                color: #555;
            }
            .section {
                margin-bottom: 25px;
                line-height: 1.7;
                font-size: 15px;
            }
        </style>
    </head>
    <body>
    """)

    # Metadata
    html.append(f"<h1>{title}</h1>")
    html.append(f"<div class='meta'><b>Authors:</b> {authors_display}</div>")
    html.append(f"<div class='meta'><b>Date:</b> {date}</div>")

    # Sections
    for section_name, content in section_summaries.items():
        html.append(f"<h2>{section_name}</h2>")
        html.append(f"<div class='section'>{content}</div>")

    html.append("""
    </body>
    </html>
    """)

    with open(output_html_path, "w", encoding="utf-8") as f:
        f.write("\n".join(html))

    print(f"✅ HTML summary saved as: {output_html_path}")

generate_html_from_summary(
    xml_path=xml_path,
    title=title,
    authors=authors,
    date=date,
    section_summaries=section_summaries
)